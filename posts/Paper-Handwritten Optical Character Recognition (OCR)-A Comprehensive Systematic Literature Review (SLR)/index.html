<!DOCTYPE html>
<html lang=zh-CN data-theme="light">
	
<script src="/js/plugins/toggleTheme.js"></script>

	<script>
		setTheme();
	</script>
	<head>
		
<title>Paper-Handwritten Optical Character Recognition (OCR)-A Comprehensive Systematic Literature Review (SLR) | Zi-Zi's Journey</title>
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0">
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<link rel="shortcut icon" type="image/x-icon" href="/images/icon/favicon.ico">
<link href="/css/plugins/print.css" media="print" rel="stylesheet" />

<link rel="stylesheet" href="/css/index.css">



<meta name="keywords" content="论文,文字识别,">
<meta name="description" content="论文阅读。">



<script src="/js/plugins/jquery.min.js"></script>


<script src="/js/plugins/hljs.min.js"></script>


<script src="/js/plugins/init.js"></script>


<script src="/js/plugins/hide.js"></script>


<script src="/js/plugins/tabs.js"></script>



    



    
<script src="/js/plugins/alert-title.js"></script>

    
<link rel="stylesheet" href="/css/plugins/github-alerts/github-base.css">

    
<link rel="stylesheet" href="/css/plugins/github-alerts/github-colors-dark-class.css">

    
<link rel="stylesheet" href="/css/plugins/github-alerts/github-colors-light.css">






    

	<meta name="generator" content="Hexo 6.1.0"></head>

	<body>
		<header class="sticky-header">
	<nav>
		<div class="nav-left">
			<a href="/" class="logo">
				<img no-lazy src="/images/headers_icon/logo.webp" alt="Quieter">
			</a>
			<ul class="breadcrumb" id="breadcrumb"></ul>
		</div>
		<div class="nav-right">
			<ul>
				
					<li>
						<a href="/">
						  主页
						</a>
					</li>
				
					<li>
						<a href="/categories">
						  类别
						</a>
					</li>
				
					<li>
						<a href="/tags">
						  标签
						</a>
					</li>
				
					<li>
						<a href="/archives">
						  归档
						</a>
					</li>
				
					<li>
						<a href="/galleries">
						  相册
						</a>
					</li>
				
					<li>
						<a href="/links">
						  链接
						</a>
					</li>
				
					<li>
						<a href="/about">
						  关于
						</a>
					</li>
								  
			</ul>
		</div>
		<div class="nav-right-close">
			<svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" width="24" height="24">
				<path fill="none" d="M0 0h24v24H0z" />
				<path d="M3 4h18v2H3V4zm0 7h18v2H3v-2zm0 7h18v2H3v-2z" fill="rgba(68,68,68,1)" />
			</svg>
		</div>

		<div class="sidebar">
    <div class="topo">
        <p>Zi-Zi's Journey</p>
    </div>
    <ul>
        
        <li>
            <a href="/">
                主页
            </a>
        </li>
        
        <li>
            <a href="/categories">
                类别
            </a>
        </li>
        
        <li>
            <a href="/tags">
                标签
            </a>
        </li>
        
        <li>
            <a href="/archives">
                归档
            </a>
        </li>
        
        <li>
            <a href="/galleries">
                相册
            </a>
        </li>
        
        <li>
            <a href="/links">
                链接
            </a>
        </li>
        
        <li>
            <a href="/about">
                关于
            </a>
        </li>
        
    </ul>
    <div class="sidebar-footer">
        
        <a target="_blank" rel="noopener" href="https://weibo.com/u/5020307235">
            <img no-lazy src="/images/bottom_icon/Weibo.webp" alt="Quieter">
        </a>
        
        <a target="_blank" rel="noopener" href="https://tieba.baidu.com/home/main?id=tb.1.ff6d2775.vFH7wrdW2ZjPCmyBHJcjnA">
            <img no-lazy src="/images/bottom_icon/Tieba.webp" alt="Quieter">
        </a>
        
        <a target="_blank" rel="noopener" href="https://space.bilibili.com/11547880">
            <img no-lazy src="/images/bottom_icon/Bilibili.webp" alt="Quieter">
        </a>
        
        <a target="_blank" rel="noopener" href="https://github.com/GZ-Metal-Cell">
            <img no-lazy src="/images/bottom_icon/github.webp" alt="Quieter">
        </a>
        
    </div>
</div>
<div class='shelter'>
    <script>
        $(function() {
            $('.nav-right-close > svg').click(function() {
                $('.sidebar').animate({
                    right: "0"
                }, 500);
                $('.shelter').fadeIn("slow");
            
                var element = $('.topo');
                element.addClass('custom-style');
            
                var links = null;
                if ("") {
                    links = "".split(',');
                } else {
                    links = "/images/random_top_img/01.webp,/images/random_top_img/02.webp,/images/random_top_img/03.webp,/images/random_top_img/04.webp,/images/random_top_img/05.webp,/images/random_top_img/06.webp,/images/random_top_img/07.webp,/images/random_top_img/08.webp,/images/random_top_img/09.webp,/images/random_top_img/10.webp,/images/random_top_img/11.webp,/images/random_top_img/12.webp,/images/random_top_img/13.webp,/images/random_top_img/14.webp,/images/random_top_img/15.webp,/images/random_top_img/16.webp,/images/random_top_img/17.webp,/images/random_top_img/18.webp,/images/random_top_img/19.webp,/images/random_top_img/20.webp,/images/random_top_img/21.webp,/images/random_top_img/22.webp,/images/random_top_img/23.webp,/images/random_top_img/24.webp,/images/random_top_img/25.webp,/images/random_top_img/26.webp,/images/random_top_img/27.webp,/images/random_top_img/28.webp,/images/random_top_img/29.webp,/images/random_top_img/30.webp,/images/random_top_img/31.webp,/images/random_top_img/32.webp,/images/random_top_img/33.webp,/images/random_top_img/34.webp,/images/random_top_img/35.webp,/images/random_top_img/36.webp,/images/random_top_img/37.webp,/images/random_top_img/38.webp,/images/random_top_img/39.webp,/images/random_top_img/40.webp,/images/random_top_img/41.webp,/images/random_top_img/42.webp,/images/random_top_img/43.webp,/images/random_top_img/44.webp,/images/random_top_img/45.webp,/images/random_top_img/46.webp,/images/random_top_img/47.webp,/images/random_top_img/48.webp,/images/random_top_img/49.webp,/images/random_top_img/50.webp,/images/random_top_img/51.webp,/images/random_top_img/52.webp,/images/random_top_img/53.webp,/images/random_top_img/54.webp,/images/random_top_img/55.webp,/images/random_top_img/56.webp,/images/random_top_img/57.webp".split(',');
                }
            
                var randomLink = links[Math.floor(Math.random() * links.length)];
                element.css('background-image', "url('" + randomLink + "')");
            });
          
            $('.shelter').click(function(e) {
                $('.sidebar').animate({
                    right: "-100%"
                }, 500);
                $('.shelter').fadeOut("slow");
            });
        });      
    </script>
</div>
	</nav>

	
		<div class="header-background"></div>
	

	<script>
		const name = 'post';
		const ul = document.querySelectorAll('.nav-right ul')[0];
		const lis = ul.querySelectorAll('li');

		if (name == 'home') {
			lis[0].classList.add('select');
		} else {
			for (let i = 0; i < lis.length; i++) {
				const li = lis[i];
				const a = li.querySelector('a');
				if (name === a.href.split('/')[3]) {
					li.classList.add('select');
				}
			}
		}
	</script>
	
	<script>
		var element = document.querySelector('.header-background');
		if(element) {
			element.classList.add('custom-style');
			var links = null;
			if("")
			{
				links = "".split(',');
			} else
			{
				links = "/images/random_top_img/01.webp,/images/random_top_img/02.webp,/images/random_top_img/03.webp,/images/random_top_img/04.webp,/images/random_top_img/05.webp,/images/random_top_img/06.webp,/images/random_top_img/07.webp,/images/random_top_img/08.webp,/images/random_top_img/09.webp,/images/random_top_img/10.webp,/images/random_top_img/11.webp,/images/random_top_img/12.webp,/images/random_top_img/13.webp,/images/random_top_img/14.webp,/images/random_top_img/15.webp,/images/random_top_img/16.webp,/images/random_top_img/17.webp,/images/random_top_img/18.webp,/images/random_top_img/19.webp,/images/random_top_img/20.webp,/images/random_top_img/21.webp,/images/random_top_img/22.webp,/images/random_top_img/23.webp,/images/random_top_img/24.webp,/images/random_top_img/25.webp,/images/random_top_img/26.webp,/images/random_top_img/27.webp,/images/random_top_img/28.webp,/images/random_top_img/29.webp,/images/random_top_img/30.webp,/images/random_top_img/31.webp,/images/random_top_img/32.webp,/images/random_top_img/33.webp,/images/random_top_img/34.webp,/images/random_top_img/35.webp,/images/random_top_img/36.webp,/images/random_top_img/37.webp,/images/random_top_img/38.webp,/images/random_top_img/39.webp,/images/random_top_img/40.webp,/images/random_top_img/41.webp,/images/random_top_img/42.webp,/images/random_top_img/43.webp,/images/random_top_img/44.webp,/images/random_top_img/45.webp,/images/random_top_img/46.webp,/images/random_top_img/47.webp,/images/random_top_img/48.webp,/images/random_top_img/49.webp,/images/random_top_img/50.webp,/images/random_top_img/51.webp,/images/random_top_img/52.webp,/images/random_top_img/53.webp,/images/random_top_img/54.webp,/images/random_top_img/55.webp,/images/random_top_img/56.webp,/images/random_top_img/57.webp".split(',');
			}
			var randomLink = links[Math.floor(Math.random() * links.length)];
			element.style.backgroundImage = "url('" + randomLink + "')";
		}
	</script>

	
<script src="/js/plugins/breadcrumb.js"></script>

	<script>
		var menus_title = [];
		
			menus_title.push({home: '主页'});
		
			menus_title.push({categories: '类别'});
		
			menus_title.push({tags: '标签'});
		
			menus_title.push({archives: '归档'});
		
			menus_title.push({galleries: '相册'});
		
			menus_title.push({links: '链接'});
		
			menus_title.push({about: '关于'});
		
		
			
				postsBreadcrumb(
					document.getElementById('breadcrumb'),
					"类别",
					"/categories",
					"学习",
					"/categories/学习",
					1
				);
			
		
	</script>
</header>

<div class="main-wrapper">
    <main class="post">
        <header class="main-header">
	
		
			
				
<link rel="stylesheet" href="/css/plugins/fancybox.css">

				
<script src="/js/plugins/fancybox.umd.js"></script>

				
<script src="/js/plugins/fancybox.js"></script>

			
			<div class="post-header-background-content">
				<ul class="post-header-tag">
					
						
							<li><a href="/tags/论文"><span>论文</span></a></li>
						
							<li><a href="/tags/文字识别"><span>文字识别</span></a></li>
						
					
				</ul>
				
				<h1>Paper-Handwritten Optical Character Recognition (OCR)-A Comprehensive Systematic Literature Review (SLR)</h1>
		
				
					<div class="post-header-desc">
						<svg t="1714702231661" class="icon" viewBox="0 0 1024 1024" version="1.1"
						xmlns="http://www.w3.org/2000/svg" p-id="1154" xmlns:xlink="http://www.w3.org/1999/xlink"
						width="20" height="20">
						<path
							d="M778.24 117.76A46.08 46.08 0 0 1 824.32 163.84v430.08c0 8.4992-4.13696 16.01536-10.50624 20.6848l-0.24576 0.2048L587.5712 846.09024a35.84 35.84 0 0 1-61.48096-25.06752v-220.9792a46.08 46.08 0 0 1 46.08-46.08l200.94976-0.02048V168.96h-522.24v686.08H389.12c13.25056 0 24.1664 10.07616 25.47712 22.97856l0.12288 2.62144c0 14.1312-11.4688 25.6-25.6 25.6h-143.36A46.08 46.08 0 0 1 199.68 860.16V163.84A46.08 46.08 0 0 1 245.76 117.76h532.48z m-26.78784 487.38304h-174.16192v178.176l174.16192-178.176z m-45.19936-169.94304a25.6 25.6 0 0 1 0 51.2H307.2a25.6 25.6 0 0 1 0-51.2h399.0528z m0-122.88a25.6 25.6 0 0 1 0 51.2H307.2a25.6 25.6 0 0 1 0-51.2h399.0528z"
							fill="#ffffff" p-id="1155"></path>
						</svg>
						<p>论文阅读。</p>
					</div>
				
		
				<div class="post-header-info">
					<svg t="1604839279282" class="icon" viewBox="0 0 1024 1024" version="1.1"
					xmlns="http://www.w3.org/2000/svg" p-id="2901" width="20" height="20">
						<path
							d="M513 956.3c-247.7 0-448-200.3-448-448S265.3 66.2 513 66.2s448 200.3 448 448-200.3 442.1-448 442.1z m0-830.9c-212.2 0-388.8 170.7-388.8 388.8C124.2 726.3 294.9 903 513 903c212.2 0 388.8-170.7 388.8-388.8S725.2 125.4 513 125.4z m0 430.2c-94.2 0-170.7-76.5-170.7-170.7S418.8 207.8 513 207.8s170.7 76.5 170.7 170.7S607.2 555.6 513 555.6z m0-289.1c-64.6 0-112 52.8-112 112s47.4 117.9 112 117.9 112-52.8 112-112-47.4-117.9-112-117.9z m0 689.8c-135.7 0-259-58.7-341.9-158.9l-11.8-17.8 11.8-17.8c76.5-117.9 206.2-188.5 347.8-188.5 135.7 0 265 64.6 341.9 182.6l11.8 17.8-11.8 17.8C778 897.1 648.7 956.3 513 956.3zM230.3 773.2C300.9 849.7 406.9 897 513 897c112 0 218.1-47.4 288.6-129.8-70.5-88.2-170.7-135.6-282.7-135.6s-218.1 53.3-288.6 141.6z"
							p-id="2902" fill="#ffffff"></path>
					</svg>
					<div class="post-header-info-author">
						<a href="/about">Zi-Zi</a>
					</div>
					
						<div class="post-header-info-categories">
							
								<a href="/categories/学习">学习</a>
							
						</div>
					
					<time>2022/12/20 14:44:00</time>
				</div>
		
				
					<div class="post-header-stat">
						<svg version="1.0" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" x="0px" y="0px"
						viewBox="0 0 200 200" enable-background="new 0 0 200 200" xml:space="preserve" width="20" height="20">
							<path fill="#FFFFFF" d="M187.2,165.6c0,2.6-2.1,4.7-4.7,4.7H17.5c-2.6,0-4.7-2.1-4.7-4.7s2.1-4.7,4.7-4.7h165.1
								C185.2,160.9,187.2,163,187.2,165.6z"/>
							<path fill="#FFFFFF" d="M17.5,29.7c2.6,0,4.7,2.1,4.7,4.7v131.2c0,2.6-2.1,4.7-4.7,4.7s-4.7-2.1-4.7-4.7V34.4
								C12.8,31.8,14.9,29.7,17.5,29.7z M77.9,91.5c1.8,1.8,1.8,4.8,0,6.6l-39.8,39.8c-1.9,1.8-4.9,1.7-6.6-0.2c-1.7-1.8-1.7-4.6,0-6.4
								l39.8-39.8C73.1,89.6,76,89.6,77.9,91.5z M169.9,70.2c1.6,2.1,1.1,5-0.9,6.5c0,0,0,0,0,0l-64.2,48.2c-2.1,1.5-5,1.1-6.6-0.9
								c-1.6-2.1-1.1-5,0.9-6.5c0,0,0,0,0,0l64.2-48.2C165.4,67.7,168.3,68.1,169.9,70.2L169.9,70.2z"/>
							<path fill="#FFFFFF" d="M104.6,124.5c-1.8,1.8-4.8,1.8-6.6,0L71.6,98.1c-1.8-1.8-1.8-4.8,0-6.6c1.8-1.8,4.8-1.8,6.6,0l26.3,26.3
								C106.4,119.6,106.4,122.6,104.6,124.5C104.6,124.4,104.6,124.4,104.6,124.5z"/>
						</svg>
		
						
							
<script src="/js/plugins/wordCount.js"></script>

							<p class="post-count">文字数：---</p>
						
		
						
							<p id="busuanzi_container_page_pv" style='display:none;'>阅读数：<span id="busuanzi_value_page_pv"></span></p>
						
					</div>
				
			</div>
		
	
</header>
        <div class="post-content article-container">
            <article class="post-content-info">
                <h1 id="%E7%9B%B8%E5%85%B3%E8%B5%84%E6%BA%90" tabindex="-1">相关资源</h1>
<ul>
<li>原文: <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2001.00139">2001.00139] Handwritten Optical Character Recognition (OCR): A Comprehensive Systematic Literature Review (SLR) (arxiv.org)</a></li>
</ul>
<h1 id="%E8%AE%BA%E6%96%87" tabindex="-1">论文</h1>


	<div class="row">
    <embed src="Paper.pdf" width="100%" height="550" type="application/pdf">
	</div>



<h2 id="abstract" tabindex="-1" id="Abstract">Abstract</h2>
<p>​        由于手写文件在人们日常生活中普遍存在（ubiquity），OCR 技术具有宝贵的实际应用价值。OCR 技术可以将各种类型的文件或图像翻译成可分析（analyzable）、可编辑（editable）和搜索（searchable）的数据。过去 10 年中，研究人员利用人工智能 / 机器学习 等工具自动分析手写（handwritten）和印刷文件（printed documents）以将其转换成为电子格式（electronic format）。这篇论文目的是总结手写文件在字符识别（character recognition）方面已经进行的研究，并提供研究方向。在本系统文献综述（Systematic Literally Review）（SLR）中，我们收集、综合和分析了 2000 年至 2018 年期间发表的有关手写 OCR 以及密切相关的主题（closely related topics）的研究文章。</p>
<p><strong>Keywords:</strong> Optical character recognition, classification, languages, feature extraction, deep learning</p>
<h2 id="1-introduction" tabindex="-1" id="1-Introduction">1 Introduction</h2>
<p>​        OCR 是一个将输入的文本（input text）转换为机器编码格式（encoded format）的系统。[^1]今天，OCR 有助于将中世纪（medieval）的手稿数字化（digitizing）[^2]，也有助于将打字文件（typewritten documents）转换成数字形式（digital form）。[^3]这使得所需信息的检索更加容易，因为人们不必翻阅成堆的文件和档案来搜索所需信息。各组织正在满足历史数据（historic data）[^4]法律文件（law documents）[^5]教育持久性（educational persistence）[^6]等方面的数字保存需求。</p>
<p>​        一个 OCR 系统主要取决于特征的提取（extraction of features）和这些特征的辨别（discrimination）/ 分类（classification）（基于模式（based on patterns））。</p>
<hr>
<p>​        Handwritten OCR 作为 OCR 一个子领域受到越来越多的关注。根据输入的数据进一步分为 offline system [^7, 8]和 online system[^9]。</p>
<ul>
<li>Offline system是一个静态系统，输入数据是以扫描图像（scanned images）的形式出现的。</li>
<li>Online system 输入的性质更加动态（more dynamic），是基于具有一定速度（having certain velocity），投影角度（projection angle），位置（position）和定位点（locus point）的笔尖运动（the movement of pen tip）。因此， online system 被认为是更复杂和先进的，因为它解决了 offline system 中存在的输入数据重叠的问题。</li>
</ul>
<hr>
<ul>
<li>
<p>最早的 OCR 系统是在 19 世纪 40 年代开发的，随着时间的推移和技术的进步，系统变得更加健壮（more robust），可以处理印刷（printed）字符和手写字符（handwritten characters），导致了 OCR 机器的商业可用性。</p>
</li>
<li>
<p>1965 年在纽约世博会推出先进的阅读机器“IBM 1287”[^10]，这是有史以来第一台能够读取手写数字的光学阅读器（optical reader）。</p>
</li>
<li>
<p>20 世纪 70 年代，研究人员专注于提高 OCR 系统的响应时间（response time）和性能（performance）。</p>
</li>
<li>
<p>自 1980 年到 2000 年，OCR 系统主要被开发（developed）并部署（deployed）在教育机构。census OCR[<sup>11]用于识别金属条上的压印字符[</sup>12]。</p>
</li>
<li>
<p>在 2000 年代初期，引入了二值化技术（binarization techniques），以数字形式保存历史文献（preserve historical documents）并为研究人员提供访问这些文件的途径[^13, 14, 15, 16]。Some of the challenges of binarization of historical documents 是使用非标准字体（nonstandard fonts）、印刷噪声（printing noise）和间距（spacing）。</p>
</li>
<li>
<p>2000 年代中期推出了多种应用程序，对不同能力的人有帮助，帮助这些人提高阅读和写作技能。</p>
</li>
</ul>
<p>​        在当前的十年中，研究人员研究了不同的机器学习方法：</p>
<ul>
<li>Support Vector Machine（SVM）</li>
<li>Random Forests（RF）</li>
<li>k Nearest Neighbor（kNN）</li>
<li>Decision Tree（DT）[^17, 18, 19]</li>
</ul>
<p>​        研究人员将这些机器学习技术与图像处理技术相结合，以提高 OCR 系统的准确性。</p>
<p>​        最近研究人员专注于开发 digitization of handwritten documents，主要基于 deep learning[^20]方法。这种模式的转变是由于集群计算（adaption of cluster computing）和 GPU 对深度学习架构适应性更好而引发的[^21]。</p>
<ul>
<li>递归神经网络（Recurrent Neural Networks，RNN）</li>
<li>卷积神经网络（Convolutional Neural Network，CNN）</li>
</ul>
<hr>
<p>​        本 SLR 不仅旨在介绍不同语言 OCR 领域的文献，还将通过突出当前 OCR 系统中需要进一步调查的薄弱领域，为新研究者强调研究方向。</p>
<p>​       This article is organized as follows.</p>
<ul>
<li>第 2 节 讨论了本文采用的综述方法（review methodology）。
<ul>
<li>综述协议（review protocol）</li>
<li>纳入和排除标准（inclusion and exclusion criteria）</li>
<li>搜索策略（search strategy）</li>
<li>选择过程（selection process）</li>
<li>质量评估标准（quality assessment criteria）</li>
<li>选定研究的元数据综合（meta data synthesis of selected studies）</li>
</ul>
</li>
<li>第 3 节 阐述了选定研究的统计数据。</li>
<li>第 4 节 提出了研究问题及其动机。</li>
<li>第 5 节 讨论了用于 handwritten OCR 的不同分类方法，还将阐述 optical character recognition 的 structural 和 statistical。</li>
<li>第 6 节 将介绍针对特定语言的不同数据库。</li>
<li>第 7 节 将介绍 OCR 中特定语言研究的研究概况。</li>
<li>第 8 节 将强调研究趋势。</li>
<li>第 9 节 将对这一综述结果进行总结，并将强调需要研究界关注的研究差距。</li>
</ul>
<h2 id="2-review-methods" tabindex="-1" id="2-Review-methods">2 Review methods</h2>
<p>​        如上所述，本系统文献综述（SLR）旨在通过制定研究问题和选择相关研究来识别和展示有关 OCR 的文献。因此，总结本综述是：</p>
<ul>
<li>总结关于不同语言手写字符识别系统的现有研究工作（机器学习技术和数据库）。</li>
<li>突出研究弱点，以便通过额外的研究消除这些弱点。</li>
<li>确定 OCR 领域内的新研究领域。</li>
</ul>
<p>​        我们将遵循 Kitchenham 等人提出的策略[^22]。根据提出的策略，讨论：</p>
<ul>
<li>综述协议（review protocol）</li>
<li>纳入和排除标准（inclusion and exclusion criteria）</li>
<li>搜索策略过程（search strategy process）</li>
<li>选择过程（selection process）</li>
<li>数据提取（data extraction）</li>
<li>合成过程（synthesis processes）</li>
</ul>
<h3 id="2.1-review-protocol" tabindex="-1" id="2-1-Review-protocol">2.1 Review protocol</h3>
<p>​        遵循系统文献综述（SLR）的理念、原则和措施[^22]，该协议确定了：</p>
<ul>
<li>研究背景（review background）</li>
<li>搜索策略（search strategy）</li>
<li>数据提取（data extraction）</li>
<li>研究问题（research questions）</li>
<li>质量评估标准（quality assessment criteria）</li>
</ul>
<p>用于研究选择和数据分析。</p>
<p>​        综述方案是 SLR 与传统文献综述或叙述性综述之间的区别所在[^22]。</p>
<h3 id="2.2-inclusion-and-exclusion-criteria" tabindex="-1" id="2-2-Inclusion-and-exclusion-criteria">2.2 Inclusion and exclusion criteria</h3>
<p>​        确保只纳入和研究相关的文章。包括：</p>
<ul>
<li>英语（English）</li>
<li>乌尔都语（Urdu）</li>
<li>阿拉伯语（Arabic）</li>
<li>波斯语（Persian）</li>
<li>印度语（Indian）</li>
<li>汉语（Chinese）</li>
</ul>
<p>​        只考虑 2000 年 1 月至 2018 年 12 月发表的研究。</p>
<h3 id="2.3-search-strategy" tabindex="-1" id="2-3-Search-strategy">2.3 Search strategy</h3>
<p>​        自动搜索策略（automatic search），选用：</p>
<ul>
<li>IEEE Explore</li>
<li>ISI Web of Knowledge,</li>
<li>ScopusâATElsevier</li>
<li>Springer.</li>
</ul>
<p>​        不选用</p>
<ul>
<li>magazine</li>
<li>working papers</li>
<li>news papers</li>
<li>books</li>
<li>blogs</li>
</ul>
<p>因为它们的质量无法得到可靠地验证。</p>
<hr>
<p>​        从主要的关键词集合中找出尽可能多的文章，如：</p>
<ul>
<li>optical character recognition</li>
<li>pattern recognition and OCR</li>
<li>pattern matching and OCR</li>
</ul>
<p>​        使用了书目管理工具 Mendeley。</p>
<h3 id="2.4-study-selection-process" tabindex="-1" id="2-4-Study-selection-process">2.4 Study selection process</h3>
<p>​        选择研究时采用了 tollgate 方法[^23]。完成自动搜索阶段后，我们开始手动搜索程序，以保证搜索结果的详尽性。</p>
<p>​        使用质量评估标准（QAC）。</p>
<h3 id="2.5-quality-assessment-criteria" tabindex="-1" id="2-5-Quality-assessment-criteria">2.5 Quality assessment criteria</h3>
<p>​        质量评估标准（QAC）基于对所选研究的总体质量（overall quality）做出决定的原则[^22]。以下标准用于评估选定研究的质量：</p>
<ul>
<li>Are topics presented in research paper relevant to the objectives of this review article?
<ul>
<li>研究论文中提出的主题是否与本文的目标相关？</li>
</ul>
</li>
<li>Does research study describes context of the research?
<ul>
<li>研究报告是否描述了研究的背景？</li>
</ul>
</li>
<li>Does research article explains approach and methodology of research with clarity?
<ul>
<li>研究文章是否清晰地解释了研究的方法和方法？</li>
</ul>
</li>
<li>Is data collection procedure explained, If data collection is done in the study?
<ul>
<li>如果研究中进行了数据收集，是否解释了数据收集的程序？</li>
</ul>
</li>
<li>Is process of data analysis explained with proper examples?
<ul>
<li>是否用适当的例子解释了数据分析的过程？</li>
</ul>
</li>
</ul>
<p>​        这 5 种 QA 模式的灵感来自[^23]。每个问题被分配 2 分，如果得分低于 5 分则不包括在研究中。</p>
<h3 id="2.6-data-extraction-and-synthesis" tabindex="-1" id="2-6-Data-extraction-and-synthesis">2.6 Data extraction and synthesis</h3>
<p>​        我们提取了所选研究的元数据（meta data）。我们使用 Mendeley 和 MS Excel 来管理这些元数据。包含：</p>
<ul>
<li>study ID（识别每项研究）</li>
<li>study title</li>
<li>authors</li>
<li>publication year</li>
<li>publishing platform（会议记录 conference proceedings，期刊 journals）</li>
<li>引文计数 citation count</li>
<li>研究背景 study context（techniques used in study）</li>
</ul>
<p>等数据被提取并被记录在 Excel 表中。</p>
<table>
<thead>
<tr>
<th>Selected Features</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td>Study identification number</td>
<td>Exclusive identity for selected research article 选定研究文章的唯一标识</td>
</tr>
<tr>
<td>Reference 参考文献</td>
<td>参考书目（Bibliographical Reference i.e）. Authors, title, publication year etc</td>
</tr>
<tr>
<td>Type of paper</td>
<td>期刊（Journal）, 会议（conference）, 研讨会（workshop）,座谈会（symposium）</td>
</tr>
<tr>
<td>Language</td>
<td>English, Urdu, Chinese, Arabic, Indian, Farsi / Persian</td>
</tr>
<tr>
<td>Citation Count 引文计数</td>
<td>Number of Citations</td>
</tr>
<tr>
<td>Technique 技术</td>
<td>特征提取和分类计数（Feature extraction and classification techniques）</td>
</tr>
</tbody>
</table>
<h2 id="3-statistical-results-from-selected-studies" tabindex="-1" id="3-Statistical-results-from-selected-studies">3 Statistical results from selected studies</h2>
<h3 id="3.1-publication-sources-overview" tabindex="-1" id="3-1-Publication-sources-overview">3.1 Publication sources overview</h3>
<ul>
<li>
<p>Majority of included studies (87) were published in research journals (61%)</p>
</li>
<li>
<p>followed by 47 publications in conference articles (33%).</p>
</li>
<li>
<p>Whereas, few (5) articles were published in workshop proceedings</p>
</li>
<li>
<p>and only 3 relevant articles were found to be presented in symposiums.</p>
</li>
</ul>
<p><img src="3.png" alt="png"></p>
<h3 id="3.2-research-citations" tabindex="-1" id="3-2-Research-citations">3.2 Research citations</h3>
<p>​        引用次数从 Google Scholar 获得。</p>
<ul>
<li>
<p>approximately 95% of the selected studies have <strong>at least one citation</strong>, excerpt few paper which are published recently in 2018.</p>
</li>
<li>
<p>Among selected studies, 33studies have more than 100 citations,</p>
</li>
<li>
<p>15studies have been cited between 61-100 times,</p>
</li>
<li>
<p>25studies were cited between 33-60 times,</p>
</li>
<li>
<p>14studies were cited between 16-30 times</p>
</li>
<li>
<p>and 46studies were cited between 1 and 15 times.</p>
</li>
</ul>
<p>​        总体而言，我们预测选定的研究引用将进一步增加，因为研究文章不断在该领域发表。</p>
<p><img src="4.png" alt="png"></p>
<p>​        这些文章可以被认为对致力于建立强大 OCR 系统的研究人员有很大的影响。</p>
<ul>
<li>Offline handwriting recognition with multidimensional recurrent neural networks.</li>
<li>Handwritten numeral databases of Indian scripts and multistage recognition of mixed numerals.</li>
<li>A novel connectionist system for unconstrained handwriting recognition.</li>
<li>Markov models for offline handwriting recognition: a survey</li>
<li>Gujarati handwritten numeral optical character reorganization through neural network.</li>
<li>Handwritten character recognition through two-stage foreground sub-sampling.</li>
<li>Deep, big, simple neural nets for handwritten digit recognition.</li>
<li>Diagonal based feature extraction for handwritten character recognition system using neural network.</li>
<li>Convolutional neural network committees for handwritten character classification.</li>
<li>Handwritten English character recognition using neural network.</li>
<li>DRAW: A recurrent neural network for image generation.</li>
<li>Online and off-line handwriting recognition: a comprehensive survey.</li>
<li>Template-based online character recognition.</li>
<li>An overview of character recognition focused on off-line handwriting.</li>
<li>IFN/ENIT-database of handwritten Arabic words.</li>
<li>Off-line Arabic character recognitionâASa review.</li>
<li>A class-modular feedforward neural network for handwriting recognition.</li>
<li>Individuality of handwriting.</li>
<li>HMM based approach for handwritten Arabic word recognition using the IFN/ENIT-database.</li>
<li>Handwritten digit recognition: benchmarking of state-of-the-art techniques.</li>
<li>Indian script character recognition: a survey.</li>
<li>Online recognition of Chinese characters: the state-of-the-art.</li>
<li>A study on the use of 8-directional features for online handwritten Chinese character recognition.</li>
<li>Offline Arabic handwriting recognition: a survey.</li>
<li>Recognition of off-line handwritten devnagari characters using quadratic classifier.</li>
<li>Connectionist temporal classification: labeling unsegmented sequence data with RNN.</li>
<li>Text-independent writer identification and verification on offline arabic handwriting.</li>
<li>A novel approach to on-line handwriting recognition based on bidirectional LSTM networks.</li>
<li>Fuzzy model based recognition of handwritten numerals.</li>
<li>Introducing a very large dataset of handwritten Farsi digits and a study on their varieties.</li>
<li>Unconstrained on-line handwriting recognition with recurrent neural networks</li>
<li>ICDAR 2013 Chinese handwriting recognition competition.</li>
<li>Automatic segmentation of the IAM off-line database for handwritten English text.</li>
</ul>
<h3 id="3.3-temporal-view" tabindex="-1" id="3-3-Temporal-view">3.3 Temporal view</h3>
<p><img src="5.png" alt="png"></p>
<p>​        我们相信 handwritten OCR 的应用领域将在未来几年进一步增加。</p>
<h3 id="3.4language-specific-research" tabindex="-1" id="3-4Language-specific-research">3.4Language specific research</h3>
<p><img src="6.png" alt="png"></p>
<ul>
<li>Total number of selected studies are 142 and out of these 142studies</li>
<li><strong>English</strong> language has the highest contribution of 45studies in the domain of character recognition</li>
<li>40studies related to <strong>Arabic</strong> language</li>
<li>26studies are on the <strong>Indian</strong> scripts</li>
<li>17 on <strong>Chinese</strong> language</li>
<li>14 on <strong>Urdu</strong> language</li>
<li>while 11studies were conducted on <strong>Persian</strong> language.</li>
<li>Some of the selected articles discussed <strong>multiple</strong> languages.</li>
</ul>
<p><img src="7.png" alt="png"></p>
<h2 id="4-research-questions" tabindex="-1" id="4-Research-questions">4 Research questions</h2>
<table>
<thead>
<tr>
<th>Research question</th>
<th>Motivation</th>
</tr>
</thead>
<tbody>
<tr>
<td>What different <strong>feature extraction</strong> and <strong>classifications methods</strong> are used for handwritten OCR?</td>
<td>确定近二十年来使用的特征提取器（feature extractors）和机器学习技术（machine learning techniques）的趋势。</td>
</tr>
<tr>
<td>What different <strong>datasets</strong> / <strong>databases</strong> are available for research purpose?</td>
<td>拥有足够的数据集是建立 OCR 系统的基本要求[^55]。</td>
</tr>
<tr>
<td>What <strong>major languages</strong> are investigated?</td>
<td>强调哪些语言通常已经被调查过。从而确定哪些语言需要更多的研究关注。</td>
</tr>
<tr>
<td>What are the <strong>new</strong> research domains in the area of OCR?</td>
<td>为新的研究项目提供指导。</td>
</tr>
</tbody>
</table>
<h2 id="5-classification-methods-of-handwritten-ocr" tabindex="-1" id="5-Classification-methods-of-handwritten-OCR">5 Classification methods of handwritten OCR</h2>
<p>​       在手写 OCR 中，算法在已知数据集上训练，并发现如何准确地对字母和数字进行分类。分类是在给定输入数据上学习模型并将其映射或标记到预定义的一个或多个类别的过程[^17]。</p>
<h3 id="5.1-artificial-neural-networks-(ann)" tabindex="-1" id="5-1-Artificial-Neural-Networks-ANN">5.1 Artificial Neural Networks (ANN)</h3>
<p>​       受生物神经元启发的结构，人工神经网络（ANN）由许多称为神经元的处理单元组成[^56]。</p>
<p>​       前馈网络/多层感知器（MLP）在 1980 年代中期重新引起了研究界的兴趣，因为当时“ Hopfield 网络”提供了理解人类记忆和计算神经元状态的方法[^58]。随着深层（多层）神经架构的出现，即递归神经网络（RNN）和卷积神经网络（CNN），神经网络已成为包括 OCR 在内的识别任务的最佳分类技术之一[^59，60，61，62]。</p>
<p><img src="8.png" alt="png"></p>
<p>​        MLP 在手写 OCR 中的早期实现是由 Shashher 等[^64]在乌尔都语上完成的。利用误差反向传播(back-propagation, BP)算法估计的连接权值，采用一个隐层，使误差平方准则（squared error criterion）最小。</p>
<p>​        最近，美国有线电视新闻卷积神经网络(CNN)报道了在字符识别任务中取得的巨大成功[<sup>67]。卷积神经网络已被广泛用于本系统文献综述所涉及的几乎所有语言的分类和识别[</sup>68, 69, 70, 71, 72, 73]。</p>
<h3 id="5.2-kernel-methods" tabindex="-1" id="5-2-Kernel-methods">5.2 Kernel methods</h3>
<p>​        许多强大的基于内核（kernel-based）的学习模型，如：</p>
<ul>
<li>Support Vector Machines（SVMs）</li>
<li>Kernel Fisher Discriminant Analysis（KFDA）</li>
<li>Kernel Principal Component Analysis (KPCA)</li>
</ul>
<p>在分类问题上已经显示出实际意义。例如在</p>
<ul>
<li>光学图案的背景 the context of optical pattern</li>
<li>文本分类 text categorization</li>
<li>时间序列预测 time-series prediction</li>
</ul>
<p>具有显著相关性。</p>
<hr>
<p>​        在支持向量机中，内核将特征向量映射到更高维的特征空间中，以找到一个超平面，该超平面以尽可能多的余量线性地将类别分开。</p>
<p>​        在深度学习方法普及之前，SVM 是手写数字识别、图像分类、人脸检测、对象检测和文本分类最强大的技术之一[^74]。核费舍尔判别分析 （KFDA） 和核主成分分析 （KPCA） 也是离线手写字符识别系统中使用的一些最重要的核方法[^75]。Boukharouba 等人[^74, 76] 使用 SVM 识别乌尔都语和阿拉伯语手写数字。</p>
<p>​        SVM 也已成功应用于</p>
<ul>
<li>图像分类 image classification</li>
<li>识别 text classification[^77、78]</li>
<li>文本分类 text classification[^79]</li>
<li>人脸和物体检测 face and object detection[^80、81]</li>
</ul>
<h3 id="5.3-statistical-methods" tabindex="-1" id="5-3-Statistical-methods">5.3 Statistical methods</h3>
<p>​        统计分类器可以是参数性（parametric）的和非参数性（non-parametric）的。</p>
<p>​        参数化分类器（parametric classifiers）在学习概念上通常很快，甚至可以在小的训练集上工作。参数化分类器的例子有：</p>
<ul>
<li>逻辑回归 Logistic Regression (LR)</li>
<li>线性判别分析 Linear Discriminant Analysis (LDA)</li>
<li>隐马尔可夫模型 Hidden Markov Model (HMM)</li>
</ul>
<p>​        非参数分类器在学习概念方面更灵活，但通常随着输入数据的大小而增加其复杂性。非参数分类器的例子有：</p>
<ul>
<li>k 临近算法 K Nearest Neighbor (KNN)</li>
<li>决策树 Decision Trees (DT)</li>
</ul>
<p>它们的参数随着训练集的大小而增加。</p>
<h4 id="5.3.1-non-parametric-statistical-methods" tabindex="-1" id="5-3-1-Non-parametric-statistical-methods">5.3.1 Non-parametric statistical methods</h4>
<p>​        用于分类的最常用和最容易训练的统计模型之一是k近邻（kNN）[^42, 82, 83]。它是一种非参数统计方法，在光学字符识别中得到了广泛的应用。 非参数识别不涉及关于数据的先验信息。</p>
<p>​        研究人员发现，使用kNN已有十多年，他们认为，在不同数据集上进行的实验中，该算法在字符识别方面取得了相对较好的性能[^61，18，83，84]。</p>
<h4 id="5.3.2-parametric-statistical-methods" tabindex="-1" id="5-3-2-Parametric-statistical-methods">5.3.2 Parametric statistical methods</h4>
<p>​        如上所述，参数技术使用固定（有限）数量的参数对概念进行建模，因为它们假设样本总体/训练数据可以通过具有一组固定参数的概率分布进行建模。在OCR研究中，一旦学习了模型的参数，通常根据一些决策规则（例如最大似然法（maximum likelihood）或贝叶斯法（Bayes））对字符进行分类[^36]。</p>
<p>​        隐马尔可夫模型（Hidden Markov Model，HMM）是 2000 年早期最常用的参数统计方法之一。在 20 世纪 90 年代，它首先被用于语音识别，然后研究人员开始将其用于光学字符的识别[^85, 86, 87]。</p>
<h3 id="5.4-template-matching-techniques" tabindex="-1" id="5-4-Template-matching-techniques">5.4 Template matching techniques</h3>
<p><img src="9.png" alt="png"></p>
<p>​        顾名思义，模板匹配（Template matching）是一种将图像（图像的一小部分）与某些预定义模板匹配的方法。通常模板匹配技术采用滑动滑动窗口方法，其中模板图像或特征在图像上滑动以确定两者之间的相似性。基于使用的相似性（或距离）度量分类得到不同物体的分类[^88]。在OCR中，模板匹配技术用于将字符与预定义模板匹配后进行分类。在文献中，使用不同的距离（相似性）度量：</p>
<ul>
<li>欧氏距离 Euclidean distance</li>
<li>曼哈顿距离 city block distance</li>
<li>相互关联性 cross correlation</li>
<li>归一化相似性 normalized correlation</li>
</ul>
<p>​        Template matching techniques 可以采用</p>
<ul>
<li>刚性形状匹配算法 rigid shape matching algorithm</li>
<li>可变形形状匹配算法 deformable shape matching algorithm</li>
</ul>
<hr>
<p>​        最适用于字符识别的方法之一是可变形模板匹配，可被进一步分为：</p>
<ul>
<li>参数化匹配 parametric matching</li>
<li>自由形式匹配 free form matching</li>
</ul>
<hr>
<p>​        刚性的模板匹配不考虑形状的变形。这种方法通常在图像与模板的特征提取（features extraction）/匹配方面（matching）起作用。OCR 中最常用的提取形状特征的方法之一是 Hough Transform，如阿拉伯语[<sup>91]和中文[</sup>92]。</p>
<p>​        刚性模板匹配的第二个子类是基于相关的匹配。在这种技术中，首先计算图像的相似性，然后基于相似性提取特定区域的特征并进行比较[^36, 93]。</p>
<h3 id="5.5-structural-pattern-recognition" tabindex="-1" id="5-5-Structural-pattern-recognition">5.5 Structural pattern recognition</h3>
<p>​        在内核方法和神经网络/深度学习方法普及之前，OCR研究界使用的另一种分类技术是结构模式识别（Structural pattern recognition）。结构模式识别的目的是根据物体的模式结构之间的关系进行分类，通常使用模式基元（pattern primitives）来提取结构（模式基元的例子见图11[^96]），即 边缘（edge）、轮廓（contours）、连接部件的几何形状（connected component geometry）等。在OCR中使用的这种图像基元之一是链码直方图（Chain Code Histogram, CCH） [^94, 95]。</p>
<p><img src="11.png" alt="png"></p>
<p>​        将 CCH 应用于 OCR 的先决条件是图像应为二进制格式，边界应明确。通常，对于手写字符识别，这种情况使得 CCH 难以使用。</p>
<p>​        在OCR的研究中，结构模型可以根据结构的上下文进一步细分，即</p>
<ul>
<li>图形方法 graphical methods</li>
<li>基于语法的方法 grammar based methods</li>
</ul>
<h4 id="5.5.1-graphical-methods" tabindex="-1" id="5-5-1-Graphical-methods">5.5.1 Graphical methods</h4>
<p>​        图（G）是数学描述连接对象之间关系的一种方式，由节点（N）和边（E）的有序对表示。通常对于OCR，E表示连接N的书写笔划弧。N和E的特殊排列定义字符/数字/字母。树（无向图，其中未定义连接方向）、有向图（其中边缘到节点的方向定义良好）在不同的研究中用于数学表示字符[^97，98]。 如上所述，书写结构组件是使用模式图元（即边缘、轮廓、连接组件几何等）提取的。这些结构之间的关系可以使用图形进行数学定义（参见图11，以了解如何使用图论对字母“R”和“e”进行建模的示例）。然后，考虑到特定的图形架构，可以使用图形相似性度量对不同的结构进行分类，即</p>
<ul>
<li>相似性洪泛算法[^99] similarity flooding algorithm</li>
<li>SimRank算法[^100] SimRank algorithm</li>
<li>图形相似性评分[^101] Graph similarity scoring</li>
<li>顶点相似性方法[^102] vertex similarity method</li>
</ul>
<p>在一项研究[^103]中，图形距离也用于分割重叠（overlapping）和连接（joined）的字符。</p>
<h4 id="5.5.2-grammar-based-methods" tabindex="-1" id="5-5-2-Grammar-based-methods">5.5.2 Grammar based methods</h4>
<p>​        在图论中，句法分析也被用来使用语法概念寻找图结构基元的相似性[104]。使用语法概念来寻找图中的相似性的好处是，这一领域的研究很深入，技术也很发达。有不同类型的基于限制规则的语法，例如</p>
<ul>
<li>无限制语法 unrestricted grammar</li>
<li>无语境语法 context-free grammar</li>
<li>语境敏感语法 context-sensitive grammar</li>
<li>常规语法 regular grammar</li>
</ul>
<p>这些语法的解释和相应的应用限制不在本调查文章的范围之内。</p>
<p>在 OCR 文献中，通常使用字符串和树来表示基于语法的模型。通过定义良好的文法，生成字符串，然后可以对其进行鲁棒分类以识别字符。树型结构也可以模拟结构元素之间的层次关系[<sup>88]。树也可以通过分析定义树的语法进行分类，从而对特定的字符进行分类[</sup>105]。</p>
<h2 id="6-datasets" tabindex="-1" id="6-Datasets">6 Datasets</h2>
<p>​        通常，为了评估和基准测试不同的OCR算法，需要/使用标准化数据库来进行有意义的比较[<sup>55]。包含足够数量数据用于训练和测试目的的数据集的可用性始终是质量研究的基本要求[</sup>106，107]。光学字符识别领域的研究主要围绕六种不同的语言展开，即英语、阿拉伯语、印度语、汉语、乌尔都语和波斯语/波斯语。因此，这些语言（如MNIST、CEDAR、CENPARMI、PE92、UCOM、HCL2000等）有公开可用的数据集。</p>
<h3 id="6.1-cedar" tabindex="-1" id="6-1-CEDAR">6.1 CEDAR</h3>
<p>​        这一传统数据集 CEDAR 由布法罗大学的研究人员于2002年开发，被认为是首批手写字符大型数据库[40]。</p>
<h3 id="6.2-mnist" tabindex="-1" id="6-2-MNIST">6.2 MNIST</h3>
<p>​        MNIST 数据集被认为是手写数字最常用/被引用的数据集之一[^42,108,30,109,110,111]。它是 NIST 数据集的子集，这就是为什么它被称为修改的 NIST 或 MNIST。</p>
<p>​        该数据集包括60,000个训练图像和10,000个测试图像。样本标准化为20 × 20灰度16图像与保留纵横比和标准化图像的大小为28 × 28。数据集大大减少了预处理和格式化所需的时间，因为它已经是规范化形式。</p>
<h3 id="6.3-ucom" tabindex="-1" id="6-3-UCOM">6.3 UCOM</h3>
<p>​        乌尔都语。</p>
<h3 id="6.4-ifn%2Fenit" tabindex="-1" id="6-4-IFN-ENIT">6.4 IFN/ENIT</h3>
<p>​        阿拉伯语。</p>
<h3 id="6.5-cenparmi" tabindex="-1" id="6-5-CENPARMI">6.5 CENPARMI</h3>
<p>​        波斯语。</p>
<h3 id="6.6-hcl2000" tabindex="-1" id="6-6-HCL2000">6.6 HCL2000</h3>
<p>​        HCL2000是一个手写汉字数据库，请参阅图17以查看示例图像。该数据集可供研究人员公开使用。该数据集包含3755个常用汉字，由1000名不同的受试者书写。该数据库的独特之处在于它包含两个子数据集，一个是手写汉字数据集，另一个是对应的作者信息数据集。提供这些信息，以便不仅可以基于字符识别，还可以基于作家的背景（如年龄、性别、职业和教育）进行研究[^117]。</p>
<p><img src="17.png" alt="png"></p>
<h3 id="6.7-iam" tabindex="-1" id="6-7-IAM">6.7 IAM</h3>
<p>​        英语手写数据库。</p>
<h2 id="7-languages" tabindex="-1" id="7-Languages">7 Languages</h2>
<p><img src="19.png" alt="png"></p>
<p>​        “世界语言处于危险之中”，世界上至少43%的语言处于危险状态[^119]。这些大量的语言需要OCR研究界的关注，以保护这一遗产免受19世纪的灭绝，或者至少建立这样的系统，将濒危语言的文档翻译成电子形式以供参考。</p>
<h3 id="7.1-english-language" tabindex="-1" id="7-1-English-language">7.1 English language</h3>
<p>​        本文解释了基于笔的计算机的现象，并通过模仿和扩展笔纸比喻实现了电子墨水自动处理的目标。为了识别字符的形状，使用了基于结构（structural）和规则（rule）的模型，如：</p>
<ul>
<li>自组织特征图 self-organized feature map（SOFM）</li>
<li>时间延迟神经网络 time delay neural network（TDNN）</li>
<li>隐马尔可夫模型 hidden markov model（HMM）</li>
</ul>
<p>​        许多字符模式的统计和结构信息可以通过神经网络（NNs）和调和马尔可夫模型（harmonic markov models，HMM）进行组合。</p>
<p>​        Connell 等人[^9]演示了一种基于模板的在线字符识别系统，该系统能够表示特定字符的不同手写风格。他们使用决策树对字符进行有效分类，准确率达到86%。</p>
<p>​        每一种语言都有特定的写作方式，并有一些不同的特点，使其与其他语言区分开来。我们相信，为了有效地识别手写和机器打印的英语文本，研究人员使用了几乎所有可用的特征提取和分类技术。这些特征提取和分类技术包括但不限于：</p>
<ul>
<li>
<p>HOG[^120]</p>
</li>
<li>
<p>双向（bidirectional） LSTM[^121]</p>
</li>
<li>
<p>方向特征 directional features[^122]</p>
</li>
<li>
<p>多层感知器 multilayer perceptron（MLP）[^123, 109, 124]</p>
</li>
<li>
<p>隐马尔可夫模型 hidden markov model（HMM）[^54, 52, 26, 61]</p>
</li>
<li>
<p>人工神经网络 Artificial neural network（ANN）[^125, 126, 127]</p>
</li>
<li>
<p>支持向量机 support vector machine（SVM）[^67, 29]</p>
</li>
</ul>
<p>​        最近的趋势正在从手工特征转向深度神经网络。卷积神经网络（CNN）架构是一类深度神经网络，已经取得了超过最先进的分类结果，特别是针对视觉刺激/输入[^128]。LeCun [^20]提出了基于多个阶段的 CNN 架构，其中每个阶段又是基于多层的。每个阶段都使用特征图，它基本上是包含像素的数组。这些像素被作为输入输入到多个隐藏层，用于特征提取和一个连接层，用于检测和分类物体[^55]。</p>
<h3 id="7.2-farsi-%2F-persian-script" tabindex="-1" id="7-2-Farsi-Persian-script">7.2 Farsi / Persian script</h3>
<h3 id="7.3-urdu-language" tabindex="-1" id="7-3-Urdu-language">7.3 Urdu language</h3>
<h3 id="7.4-chinese-language" tabindex="-1" id="7-4-Chinese-language">7.4 Chinese language</h3>
<p>​        在完成研究选择过程后，我们的研究包括17篇关于中文OCR系统的研究论文（参考第2.4节和第3.4节）。最早的中文研究之一是由Fu等人在2000年完成的[^142]。研究人员使用自我增长的基于概率决策的神经网络（self-growing probabilistic decision-based neural networks, SPDNNs）来开发一个用户适应模块，用于字符识别和个人适应。结果在十个适应周期（adapting cycles）内，识别准确率达到了90.2%。</p>
<hr>
<p>​        后来在 2005 年，Cheng 和 Fujisawa [^67]对将基于特征向量的分类方法应用于字符识别的比较研究发现，当样本量较大时，人工神经网络（ANN）和支持向量机（SVM）等判别性分类器比统计性分类器的分类准确率更高。然而，在该研究中，SVM 在许多实验中表现出比神经网络更好的准确性。</p>
<hr>
<p>​        在另一项研究中，Bai 和 Huo [^45]评估了使用 8 个方向的特征来识别在线手写的汉字。经过一系列的处理步骤，使用一个派生滤波器在均匀采样的位置上提取了模糊的方向特征，形成了一个 512 维的原始特征向量。与早期使用 4 方向特征的方法相比，这种方法的性能要好得多。</p>
<hr>
<p>​        2009 年，Zhang [^117]提出 HCL2000，一个大规模的手写汉字数据库。它存储了 3755 个常用的汉字以及 1000 个不同书写者的信息。HCL2000 使用三种不同的算法进行评估：</p>
<ul>
<li>线性判别分析（Linear Discriminant Analysis, LDA）</li>
<li>位置保护投影（Locality Preserving Projection, LPP）</li>
<li>边际费雪分析（Marginal Fisher Analysis, MFA）。</li>
</ul>
<p>在分析之前，一个最近的邻居分类器将输入图像分配到一个字符组。实验结果显示 MFA 和 LPP 比 LDA 更好。</p>
<hr>
<p>​        Yin等人[^53]提出了ICDAR 2013竞赛，该竞赛收到了27个系统，涉及5个任务：提取特征数据的AS分类、在线/离线孤立字符识别和在线/离线手写文本识别。系统中使用的技术包括：</p>
<ul>
<li>LDA</li>
<li>修正二次判别函数（Modified quadratic discriminant function, MFQD）</li>
<li>复合马哈拉诺比斯函数（Compound Mahalanobis Function, CMF）</li>
<li>卷积神经网络（convolutional neural network, CNN）</li>
<li>多层感知器（multilayer perceptron, MLP）</li>
</ul>
<p>研究发现，基于神经网络的方法被证明在识别孤立字符和手写文本方面效果更好。</p>
<hr>
<p>​        2018 年，汉字研究人员使用神经网络识别 CAPTCHA（完全自动化的公共图灵测试来区分计算机和人类）识别 [^70]、医疗文件识别 [^143]、车牌识别 [^144] 和历史文件中的文本识别 [^71]。研究人员在这些研究中使用了</p>
<ul>
<li>卷积神经网络 (CNN) [^70、71]</li>
<li>卷积递归神经网络 (CRNN) [^143]</li>
<li>单深度神经网络 (SDNN) [^144]。</li>
</ul>
<h3 id="7.5-arabic-script" tabindex="-1" id="7-5-Arabic-script">7.5 Arabic script</h3>
<h3 id="7.6-indian-script" tabindex="-1" id="7-6-Indian-script">7.6 Indian script</h3>
<h2 id="8-research-trends" tabindex="-1" id="8-Research-trends">8 Research trends</h2>
<p>​        最近，光学字符识别领域的研究已转向深度学习方法[^166, 167]，很少或根本不强调手工制作的特征。我们可以看到神经网络，特别是 CNN 被广泛用于光学字符的识别。然而，SVM、HMM、SIFT 等传统技术也正在与 CNN 结合使用。</p>
<h2 id="9-conclusion-and-future-work" tabindex="-1" id="9-Conclusion-and-future-work">9 Conclusion and future work</h2>
<h3 id="9.1-conclusion" tabindex="-1" id="9-1-Conclusion">9.1 Conclusion</h3>
<p>​        光学字符识别已经出现了八十年。然而，最初识别光学字符的产品大多是由大型技术公司开发的。机器学习和深度学习的发展使个体研究人员能够开发出算法和技术，这些算法和技术能够更准确地识别手写稿件。</p>
<hr>
<p>​        在这篇文献综述中，我们系统地提取和分析了六种广泛使用的语言的研究成果。我们发现有些技术在一种文字上比在另一种文字上表现得更好，例如，多层感知机分类器在 Devanagri 和孟加拉数字上表现得更准确[^25,129] ，但在其他语言上表现得更平均[^ 123,109,124]。这种差异可能是由于特定技术对不同风格的字符和数据集质量建模的方式不同。</p>
<hr>
<p>​        大多数已发表的研究都提出了一种语言甚至一种语言子集的解决方案。公开可用的数据集还包括彼此很好地对齐的刺激，并且未能结合与真实生活场景很好地对应的示例，即：</p>
<ul>
<li>书写风格 writing styles</li>
<li>扭曲的笔画 distorted strokes</li>
<li>可变的字符粗细 variable character thickness</li>
<li>照明 illumination[^183]</li>
</ul>
<hr>
<p>​        还观察到研究人员越来越多地使用卷积神经网络（CNN）来识别手写和机器打印的字符。这是因为基于CNN的架构非常适合输入为图像的识别任务。CNN最初用于图像中的物体识别任务，例如 ImageNet 大规模视觉识别挑战（ILSVRC）[^184]。AlexNet  [^185]，GoogLeNet [^186] 和 ResNet  [^187]是一些基于CNN的架构，广泛用于视觉识别任务。</p>
<h3 id="9.2-future-work" tabindex="-1" id="9-2-Future-work">9.2 Future work</h3>
<p>​        如第 7 节所述，OCR领域的研究通常是针对一些最广泛使用的语言进行的。这部分是由于其他语言上的数据集不可用。未来的研究方向之一是对广泛使用的语言以外的语言进行研究，即。区域语言和濒危语言。这有助于保护脆弱社区的文化遗产，也将对加强全球协同作用产生积极影响。</p>
<hr>
<p>​        另一个需要研究界关注的研究问题是建立能够在日常生活场景中识别屏幕上不同条件下的字符和文本的系统，例如字幕或新闻字幕中的文本、招牌上的文本、广告牌上的文本等。这是“识别/分类/野外文本”的领域。这是一个需要解决的复杂问题，因为这种场景的系统需要处理背景杂波、可变照明条件、可变相机角度、失真字符和可变书写风格[^183]。</p>
<hr>
<p>​        为了构建“text in the wild”的强大系统，研究人员需要提出具有挑战性的数据集，这些数据集足够全面，能够包含所有可能的字符变化。一个这样的努力是[^188]。在另一次尝试中，研究界推出了“ICDAR 2019：多语言场景文本检测和识别的 Robustreading 挑战”[^189]。这一挑战的目的是邀请研究人员，为日常生活或“in the wild”场景中的多语言文本识别提出鲁棒系统。最近发布了该挑战的报告，挑战中不同任务的获胜方法都基于不同的深度学习架构，例如CNN、RNN 或 LSTM。</p>
<hr>
<p>​        已发表的研究报告提出了各种 OCR 系统，但需要改进的一个方面是研究的商业化。研究的商业化将有助于为 OCR 构建低成本的真实系统，该系统可以将大量宝贵的信息转化为可搜索/数字数据[^190]。</p>

            </article>
            
	<div class="rightside">
	
		<div class="rightside-button" id="js-aside">
			<span>
				<img no-lazy src="/images/icon/aside.png" class="rightside-button-icon" alt="Icon">
			</span>
		</div>
		<script>
			$("#js-aside").click(function () {
				onShowAsideButton();
			});
		</script>
	
	<div class="rightside-button" id="js-toggle_theme">
		<span>
			<img no-lazy src="/images/icon/toggle_theme.png" class="rightside-button-icon" alt="Icon">
		</span>
	</div>

	
<script src="/js/plugins/goto_position.js"></script>

	
	<div class="rightside-button" id="js-go_top">
		<span>
			<img no-lazy src="/images/icon/go_top.png" class="rightside-button-icon" alt="Icon">
		</span>
	</div>
	<div class="rightside-button" id="js-go_bottom">
		<span>
			<img no-lazy src="/images/icon/go_bottom.png" class="rightside-button-icon" alt="Icon">
		</span>
	</div>

	<script>
		setToggleThemeButtonListener();
	</script>
	<script>
		$('#js-go_top')
		.gotoPosition( {
			speed: 300,
			target: 'top',
		} );
		$('#js-go_bottom')
		.gotoPosition( {
			speed: 300,
			target: 'bottom',
		} );
	</script>
</div>


<div class="post-bottom">
    
        <div class="post-paging">     
            <div class="post-paging-last">
                
                    <a href="/posts/Diary-%E7%BB%93%E6%9D%9F%E4%BA%86%E7%96%AB%E5%BE%80%E6%83%85%E6%B7%B1%E7%9A%842022/">
                        上一篇：Diary-结束了疫往情深的2022
                    </a>
                
            </div>
            <div class="post-paging-next">
                
                    <a href="/posts/DIP-Python%20tutorials%20for%20image%20processing%20and%20machine%20learning(73-78)-U-net/">
                        下一篇：DIP-Python tutorials for image processing and machine learning(73-78)-U-net
                    </a>
                
            </div>
        </div>
    
    
    
        
            <div class="giscus comments"></div>
            <script>
                var scriptElement = document.createElement('script');
                scriptElement.src = 'https://giscus.app/client.js';
                scriptElement.setAttribute('data-repo', 'GZ-Metal-Cell/GZ-Metal-Cell.github.io');
                scriptElement.setAttribute('data-repo-id', 'R_kgDOIHLEOQ');
                scriptElement.setAttribute('data-category', 'Announcements');
                scriptElement.setAttribute('data-category-id', 'DIC_kwDOIHLEOc4CcVwP');
                scriptElement.setAttribute('data-mapping', 'title');
                scriptElement.setAttribute('data-strict', '1');
                scriptElement.setAttribute('data-reactions-enabled', '');
                scriptElement.setAttribute('data-emit-metadata', '0');
                scriptElement.setAttribute('data-input-position', 'bottom');
                scriptElement.setAttribute('data-theme', localStorage.getItem('theme') === 'light' ? 'light' : 'dark_high_contrast');
                scriptElement.setAttribute('data-lang', 'zh-CN');
                
                scriptElement.setAttribute('crossorigin', 'anonymous');
                scriptElement.async = true;
                document.head.appendChild(scriptElement);
            </script>
        
    
</div>
        </div>
    </main>
    
        <aside class="main-aside">
    
<script src="/js/widgets/aside.js"></script>

    <script>
        showAside();
    </script>

    <div class="aside-top">
        <div class="aside-top-about aside-card">
            <a href="/about" class="aside-top-about-portrait">
                <img no-lazy src="/about/portrait.png" alt="Q">
            </a>
            <div class="aside-top-about-info">
                <span class="author"> Zi-Zi</span>
                <span class="description">不以物喜，不以己悲。</span>
            </div>              
            <div class="aside-top-about-site">
                <a href="/categories" class="aside-top-about-site-item">
                    <span class="title">类别</span>
                    <span class="count">5</span>
                </a>
                <a href="/tags" class="aside-top-about-site-item">
                    <span class="title">标签</span>
                    <span class="count">121</span>
                </a>
                <a href="/archives" class="aside-top-about-site-item">
                    <span class="title">归档</span>
                    <span class="count">436</span>
                </a>
            </div>
            <div class="aside-top-about-contact">
                
                    
                        <a target="_blank" rel="noopener" href="https://weibo.com/u/5020307235">
                            <img no-lazy src="/images/bottom_icon/Weibo.webp" alt="Quieter">
                        </a>
                    
                        <a target="_blank" rel="noopener" href="https://tieba.baidu.com/home/main?id=tb.1.ff6d2775.vFH7wrdW2ZjPCmyBHJcjnA">
                            <img no-lazy src="/images/bottom_icon/Tieba.webp" alt="Quieter">
                        </a>
                    
                        <a target="_blank" rel="noopener" href="https://space.bilibili.com/11547880">
                            <img no-lazy src="/images/bottom_icon/Bilibili.webp" alt="Quieter">
                        </a>
                    
                        <a target="_blank" rel="noopener" href="https://github.com/GZ-Metal-Cell">
                            <img no-lazy src="/images/bottom_icon/github.webp" alt="Quieter">
                        </a>
                    
                
            </div>
        </div> 

        
    </div>

    <div class="aside-bottom">
        
            <script>
                
                    const tocCollapsed = true;
                
                
                    const tocDepth = 6;
                
                var headerString = '';
                for (let i = 1; i <= tocDepth; i++) {
                    if (i === 1) {
                        headerString += 'h1';
                    } else {
                        headerString += ', h' + i;
                    }
                }
                hbeToc();
            </script>
            <div class="aside-bottom-toc aside-card">
                <div class="aside-bottom-toc-title">
                    <h1>目录</h1>
                    <span class="toc-percentage"></span>
                </div>
                <ol class="aside-bottom-toc-content"></ol>
            </div>
        
    </div>
</aside>
    
</div>
		<footer>
	<div class="content">
		
			<span>©2022-2025&nbsp;By&nbsp;<a href="/about">Zi-Zi</a>.</span>
		
		<span><a target="_blank" rel="noopener" href="https://hexo.io/">Hexo</a> theme by <a target="_blank" rel="noopener" href="https://github.com/GZ-Metal-Cell/hexo-theme-quieter">Quieter</a>.</span>
		
			<span style="display: flex;">
				<img no-lazy alt="icp" src="/images/icp_icon.png" style="width: 16px; height: 16px;">
				<a href="https://icp.gov.moe/?keyword=20241647" target="_blank">萌 ICP 备 20241647 号</a>
			</span>
		
	</div>

	
<script src="/js/plugins/ref.js"></script>

	
<script src="/js/plugins/highlight_tools.js"></script>

	<script>
		const  COPY_ICON = "/images/icon/copy.png";
		const CLOSE_CODE_BLOCK_ICON = "/images/icon/close_code_block.png";
		const HIGHLIGHT_SHRINK = "";
		const HIGHLIGHT_HEIGHT_LIMIT = "";
	</script>

	
	
	<!-- Analytics -->

    
        <!-- Busuanzi Analytics -->
        <script async src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script>
    
    
        <!-- Baidu Analytics -->
        <script defer>
            var _hmt = _hmt || [];
            (function () {
                var hm = document.createElement("script");
                hm.src = "https://hm.baidu.com/hm.js?e57cf62289f84322ebff116e8b3d343e";
                var s = document.getElementsByTagName("script")[0];
                s.parentNode.insertBefore(hm, s);
            })();
        </script>
    


	

	
		
			
				<link rel="stylesheet" href="/css/plugins/katex/katex.min.css">
				<script src="/js/plugins/copy-tex.js"></script>
			
		
	

    
		
<link rel="stylesheet" href="/css/plugins/textIndent.css">

		
<script src="/js/plugins/textIndent.js"></script>

	

	
	
	
		<script>
			if (typeof init === 'function') {
				init();
			}
		</script>
	

	
		
	

	

	<!--
		
<script src="/js/plugins/jquery.pjax.min.js"></script>

		<script>
			$(document).pjax('a[target!=_blank]', 'main', {
				fragment: 'main',
				timeout: 8000
			});

			$(document).on('pjax:complete', function() {
			});
		</script> 
	-->
	<script>
		console.log('\n %c Hexo-Quieter 主题 %c https://github.com/GZ-Metal-Cell/hexo-theme-quieter \n', 'color: #fadfa3; background: #030307; padding:5px 0;', 'background: #fadfa3; padding:5px 0;')
	</script>
</footer>
	</body>

	<!-- Hexo-Quieter 主题  https://github.com/GZ-Metal-Cell/hexo-theme-quieter -->
</html>

